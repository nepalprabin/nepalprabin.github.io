[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume",
    "section": "",
    "text": "I am currently a Computer Science graduate student at the University of South Dakota."
  },
  {
    "objectID": "cv.html#projects",
    "href": "cv.html#projects",
    "title": "Resume",
    "section": "Projects",
    "text": "Projects\n\n1. Stanford NLP Lecture Transcription using OpenAI’s Whisper\nWhisper is an automatic speech recognition (ASR) model trained on hours of multilingual and multitask supervised data. It is implemented as an encoder-decoder transformer architecture where audio are splitted into 30 seconds of chunks, converted into a log-Mel spectrogram, and then passed into an encoder. The decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation. For more info about whisper, read here.\nI used whisper model to transcribe Stanford NLP lectures into corresponding text captions. Here is the result of the transcribed lectures. This web app is build using Flask and deployed on AWS EC2 instance. You can find transcribed audio file in the form of text here."
  },
  {
    "objectID": "cv.html#custom-named-entity-recognizer-for-clinical-data",
    "href": "cv.html#custom-named-entity-recognizer-for-clinical-data",
    "title": "Resume",
    "section": "2. Custom Named Entity Recognizer for clinical data",
    "text": "2. Custom Named Entity Recognizer for clinical data\nNamed Entity Recognition (NER) is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named entities in text.\nI have developed a custom named entity recognition (NER) model for clinical data using the spacy framework and deployed it using Streamlit. The model is capable of identifying various entities such as diseases, treatments, medications, and anatomical locations from clinical text data. The model classifies entities based on three classes: ‘MEDICINE’, “MEDICALCONDITION”, and “PATHOGEN”. The dataset was used from kaggle. You can try the application on this link"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Resume",
    "section": "Skills",
    "text": "Skills\nLanguages: Python, JavaScript\nDatabases: MySQL, Postgresql, MongoDB\nFrameworks and Libraries: Pytorch, Tensorflow, Huggingface Transformers, OpenCV, SpaCy, Django, FastAPI, Vuejs\nTools: Git, Linux, Docker, AWS (EC2, S3, Lambda function, Sagemaker), Hydra, Model Packaging (ONXX)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently a Computer Science graduate student at the University of South Dakota.\n\nInterests\n\nSoftware Development\nMachine Learning\nData Science\nLarge Language Models (LLMs)\nSystem Design\n\n\n\nEducation\n\n\nUniversity of South Dakota\nMS in Computer Science\n\n\nSouth Dakota, SD\nMay 2023\n\n\n\n\nVisvesvaraya Technological University (VTU)\nBachelor in Computer Science and Engineering\n\n\nBangalore, India\nGraduated July 2019\n\n\n\n\nExperience\n\n\nUniversity of South Dakota\n\n\nAugust 2022 - present\n\n\nGraduate Assistant/Data Analyst\n\nMachine Learning: Creating a predictive model using machine learning algorithms to identify at-risk children and prevent incidents of abuse.\nScripting: Developing automated scripts using Python that merges data from different sources and reducing manual time of 1 hour to 10 minutes.\nAnalysis: Analyzing data to develop sustainable solution to reduce child sexual abuse and maltreatment in South Dakota.\nReporting: Implementing interactive dashboards and reports using Tableau to analyze child abuse rates.\n\n\n\nEnsemble Matrix\n\n\nNov 2020 - Dec 2021\n\n\nPython Software Engineer\n\nAPIs: Developed backend APIs that support customer-facing product features using Django, GraphQL, and EC2.\nAsync workloads: Worked on backend systems that handle asynchronous workloads such as data ingestion and egestion from third-party ecommerce systems, using ECS, SQS, and Airflow.\nInfrastructure as Code: Utilized Infrastructure as Code to model existing and new AWS resources as code for faster iterative deployments, specifically using AWS EC2.\nCI/CD: Designed and implemented a comprehensive CI/CD pipeline for multiple projects from scratch, utilizing AWS EC2 and Github Actions.\nTests: Established code testing procedures from scratch, including linting and type checking, using pytest and SQLAlchemy.\nStructured Logging: Revamped logging across multiple projects to improve bug discoverability, utilizing Datadog and Sentry for structured logging.\nReporting: Added analytics report generation tools for customers to gain more insights into business metrics, using Pandas and Django for ETL and reporting.\n\n\n\nBitpoint Pvt. Ltd.\n\n\nAugust 2019 - Nov 2020\n\n\nPython Software Engineer\n\nAPIs: Created back-end APIs for medical entrance preparation application.\n3rd Party Integrations: Created payment integrations to let users buy subscriptions for the application.\nProduct Features: Worked on the back end to add features like progress tracking, question recommendation, comments, notification emails, etc. (Django, celery, SqlAlchemy, Postgres, pytest).\nAuthorization: Added JWT authorization to user API.\n\n\n\nTeaching\n\nAchiever Gropus\n\n\nPython Data Analysis and Web development using Django\n\n\n2021"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Prabin's Resume"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Whisper is an automatic speech recognition (ASR) model trained on hours of multilingual and multitask supervised data. It is implemented as an encoder-decoder transformer architecture where audio are splitted into 30 seconds of chunks, converted into a log-Mel spectrogram, and then passed into an encoder. The decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation. For more info about whisper, read here.\nI used whisper model to transcribe Stanford NLP lectures into corresponding text captions. Here is the result of the transcribed lectures. This web app is build using Flask and deployed on AWS EC2 instance. You can find transcribed audio file in the form of text here."
  },
  {
    "objectID": "projects.html#custom-named-entity-recognizer-for-clinical-data",
    "href": "projects.html#custom-named-entity-recognizer-for-clinical-data",
    "title": "Projects",
    "section": "2. Custom Named Entity Recognizer for clinical data",
    "text": "2. Custom Named Entity Recognizer for clinical data\nNamed Entity Recognition (NER) is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named entities in text.\nI have developed a custom named entity recognition (NER) model for clinical data using the spacy framework and deployed it using Streamlit. The model is capable of identifying various entities such as diseases, treatments, medications, and anatomical locations from clinical text data. The model classifies entities based on three classes: ‘MEDICINE’, “MEDICALCONDITION”, and “PATHOGEN”. The dataset was used from kaggle. You can try the application on this link"
  },
  {
    "objectID": "posts/2020-04-24-alexnet-architecture-explained.html",
    "href": "posts/2020-04-24-alexnet-architecture-explained.html",
    "title": "AlexNet Architecture Explained",
    "section": "",
    "text": "AlexNet famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% vs 26.2%(second place) error rates). Here is the link to original paper.\nMajor highlights of the paper\n\nUsed ReLU instead of tanh to add non-linearity.\nUsed dropout instead of regularization to deal with overfitting.\nOverlap pooling was used to reduce the size of the network.\n\n1. Input\nAlexNet solves the problem of image classification with subset of ImageNet dataset with roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. The input is an image of one of 1000 different classes and output is a vector of 1000 numbers.\nThe input to AlexNet is an RGB image of size 256*256. This mean that all the images in training set and test images are of size 256*256. If the input image is not 256*256, image is rescaled such that shorter size is of length 256, and cropped out the central 256*256 patch from the resulting image.\n\n\n\n\nsource\n\nThe image is trained with raw RGB values of pixels. So, if input image is grayscale, it is converted into RGB image . Images of size 257*257 were generated from 256*256 images through random crops and it is feed to the first layer of AlexNet.\n2. AlexNet Architecture\nAlexNet contains five convolutional layers and three fully connected layers - total of eight layers. AlexNet architecture is shown below:\n\n\n\n\nAlexNet Architecture\n\nFor the first two convolutional layers, each convolutional layers is followed by a Overlapping Max Pooling layer. Third, fourth and fifth convolution layers are directly connected with each other. The fifth convolutional layer is followed by Overlapping Max Pooling Layer, which is then connected to fully connected layers. The fully connected layers have 4096 neurons each and the second fully connected layer is feed into a softmax classifier having 1000 classes.\n2.1) ReLU Non-Linearity:\nThe standard way of introducing nonlinearity is using tanh: f(x) = tanh(x) where f is a function of input x or using f(x) = (1+e-x)-1.\nThese are saturating nonlinearities which are much slow than non-saturating nonlinearity f(x) = max(0, x), in terms of training time with gradient descent.\n\n\n\n\nfig. (Tanh and Relu activation functions)\n\nSaturating nonlinearities: These functions have a compact range, meaning that they compress the neural response into a bounded subset of the real numbers. The LOG compresses inputs to outputs between 0 and 1, the TAN H between -1 and 1. These functions display limiting behavior at the boundaries.\nTraining network with non-saturating nonlinearity is faster than that of saturating non-linearity.\n2.2) Overlapping Pooling:\nMax Pooling layers help to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned. Max Pooling helps to reduce overfitting. Basically, it uses a max operation to pool sets of features, leaving us with a smaller number of them. Max Pooling and Overlapping is same except except the adjacent windows over which the max is computed overlap each other.\n\n“A pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighbourhood of size z*z centered at the location of pooling unit. If we set s=z, we obtain traditional local pooling. If we set s < z, we obtain overlapping pooling.\nAlexNet Paper (2012)\n\nThe overlapping pooling reduces the top-1 and top-5 error rates by 0.4% and 0.3% compared to non-overlapping pooling, thus finding it very difficult to overfit.\n2.3) Reducing Overfitting\nVarious techniques are applied to reduce overlapping\nData Augmentation\nThe most common way to reduce overfitting on image data is data augmentation. It is a strategy to significantly increase the diversity of data available for training the models without collecting new data. Data augmentation includes techniques such as Position augmentation (cropping, padding, rotating, translation, affine transformation), color augmentation(Brightness, contrast saturation, hue) and many other. AlexNet employ two distinct forms of data augmentation.\nThe first form of data augmentation is translating the image and horizontal reflections. This is done by extracting random 224*224 patches from 256*256 images and training network on these patches. The second form of data augmentation consists of altering the intensities of RGB channel in training images.\nDropout\nDropout is a regularization technique to reduce overfitting and improving generalization of deep neural networks. ‘Dropout’ refers to dropping out units(hidden and visible) in a neural network. We can interpret dropout as the probability of training  a given node in a layer, where 1.0 means no dropout and 0.5 means 50% of hidden neurons are ignored.\n\n\n\n\nDropout\n\nReferences:\n\nImageNet Classification with Deep Convolutional Neural Networks  by Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, 2012\nhttps://www.learnopencv.com/understanding-alexnet/"
  },
  {
    "objectID": "posts/2022-10-19-text-summarization-nlp.html",
    "href": "posts/2022-10-19-text-summarization-nlp.html",
    "title": "Text Summarization NLP",
    "section": "",
    "text": "What is text summarization?\n\n\nText summarization is one of the Natural Language Processing (NLP) tasks where documents/texts are shortened automatically while holding the same semantic meaning. Summarization process generates short, fluent and accurate summary of the long documents. The main idea of text summarization is to find the subset of the most important information from the entire document and present it in a human readable format. Text summarization has its application in other NLP tasks such as Question Answering (QA), Text Classification, Text Generation and other fields.\n\nTypes of summarization\n\nBased on how the texts are extracted from the documents, the summarization process can be divided into two types: extractive summarization and abstractive summarization.\n\n\nExtractive Summarization\n\nExtractive summarization picks up the most important sentences directly from the documents and forms a coherent summary. This is done using a scoring function. Extractive summarization takes a sentence as an input and produces a probability vector as the output. This probability vector represents the probability of a sentence being included in the summary.\n\n\nImplementing extractive summarization based on word frequency\n\nWe can implement extractive summarization using word frequency in five simple steps:\na. Creating word frequency table\nWe count the frequency of the words present in the text and create a frequency table which is a dictionary to store the count. While creating the frequency table, we do not account for the stop words present in the text and remove those words.\ndef frequency_table(text):\n    # all unique stopwords of english\n    stop_words = set(stopwords.words(\"english\"))\n    words = word_tokenize(text)\n    freq_table = dict()\n\n    # creating frequency table to keep the count of each word\n    for word in words:\n        word = word.lower()\n        if word in stop_words:\n            continue\n        if word in freq_table:\n            freq_table[word] += 1\n        else:\n            freq_table[word] = 1\n\n    return freq_table\nb. Tokenizing the sentences\nHere we tokenize the sentences using NLTK’s sent_tokenize() method. This separates paragraphs into individual sentences.\ndef tokenize_sentence(text):\n    return sent_tokenize(text)\nc. Scoring the sentences using term frequency\nHere, we score a sentence by its words, by adding frequency of every word present in the sentence excluding stop words. One downside of this approach is, if the sentence is long, the value of frequency increases.\ndef term_frequency_score(sentence, freq_table):\n    # dictionary to keep the score\n    sentence_value = dict()\n\n\n    for sentence in sentences:\n        for word, freq in freq_table.items():\n            if word in sentence.lower():\n                if sentence in sentence_value:\n                    sentence_value[sentence] += freq\n                else:\n                    sentence_value[sentence] = freq\n    return sentence_value\nd. Finding the threshold score\nAfter calculating the term frequency, we calculate the threshold score.\ndef calculate_average_score(sentence_value):\n    # To compare the sentences within the text, we assign a score.\n    sum_values = 0\n    for sentence in sentence_value:\n        sum_values += sentence_value[sentence]\n\n    # Calculating average score of the sentence. This average score can be a good threshold.\n    average = int(sum_values / len(sentence_value))\n\n    return average\ne. Generating the summary based on the threshold value\nBased on the threshold value, we generate the summary of the text.\ndef create_summary(sentences, sentence_value, threshold):\n  # Applying the threshold value and storing sentences in an order into the summary.\n  summary = ''\n  for sentence in sentences:\n      if (sentence in sentence_value) and (sentence_value[sentence] > (1.2 * threshold)):\n          summary += \" \"+ sentence\n  return summary\n\n\nAbstractive Summarization\n\nIn abstractive summarization, the model forms its own phrases and sentences to provide a consistent summary. Abstractive summarization does not simply copy the sentences to form the summary but create new phrases that are relevant to the original document. This summarization technique uses deep learning techniques (like seq2seq) to paraphrase and shorten the original document.\n\n\nAbstractive Summarization using Transformers\n\nTransformers is an architecture which uses attention mechanisms to solve sequence to sequence problems while solving long term dependencies. Ever since it was introduced in 2017, transformers have been widely used in various NLP tasks such as text generation, question answering, text classification, language translation and so on. The transformer architecture consists of encoder and decoder parts. The encoder component consists of 6 encoders each of which consists of two sub layers: self-attention and feed forward networks. The input text is first converted into vectors using text embedding methods. Then the vector is passed into the self attention layer and the output from the self attention layer is passed through the feed forward network. The decoder also consists of both self attention and feed forward network layer. An additional layer is present in between these components which is an attention layer that helps the decoder to focus on the relevant parts of the input sentence.\n\n\n\n\nFig. Transformer architecture (from original paper)\n\nHuggingface Transformers provide various pre-trained models to perform NLP tasks. It provides APIs and tools to download and train state-of-the-art pre-trained models. Not only NLP, huggingface supports Computer Vision tasks like image classification, object detection and segmentation, audio classification and recognition, and multimodal tasks like table question answering, optical character recognition, and many more.\n\nBasic transformer pipeline for summarization\n\nHuggingface transformers provide an easy to use model for inference using pipeline. These pipelines are the objects that hide complex code and provide a simple API to perform various tasks.\nfrom transformers import pipeline\n\nclassifier = pipeline(\"summarization\")\ntext = \"\"\"Acnesol Gel is an antibiotic that fights bacteria. It is used to treat acne, which appears as spots or pimples on your face, chest or back. This medicine works by attacking the bacteria that cause these pimples.Acnesol Gel is only meant for external use and should be used as advised by your doctor. You should normally wash and dry the affected area before applying a thin layer of the medicine. It should not be applied to broken or damaged skin.  Avoid any contact with your eyes, nose, or mouth. Rinse it off with water if you accidentally get it in these areas. It may take several weeks for your symptoms to improve, but you should keep using this medicine regularly. Do not stop using it as soon as your acne starts to get better. Ask your doctor when you should stop treatment. Common side effects like minor itching, burning, or redness of the skin and oily skin may be seen in some people. These are usually temporary and resolve on their own. Consult your doctor if they bother you or do not go away.It is a safe medicine, but you should inform your doctor if you have any problems with your bowels (intestines). Also, inform the doctor if you have ever had bloody diarrhea caused by taking antibiotics or if you are using any other medicines to treat skin conditions. Consult your doctor about using this medicine if you are pregnant or breastfeeding.\"\"\"\nclassifier(text)\nResult:\n[{'summary_text': ' Acnesol Gel is an antibiotic that fights bacteria that causes pimples . It is used to treat acne, which appears as spots or pimples on your face, chest or back . The medicine is only meant for external use and should be used as advised by your doctor .'}]\nThe pipeline() takes the name of the task to be performed (if we want to perform a question-answering task, then we can simply pass “question-answering” into the pipeline() and it automatically loads the model to perform the specific task.\n\nFine-tuning summarization model for medical dataset\n\nSummarization using abstractive technique is hard as compared to extractive summarization as we need to generate new text as the output. Different architectures like GTP, T5, BART are used to perform summarization tasks. We will be using the PubMed dataset. It contains datasets of long and structured documents obtained from PubMed OpenAccess repositories. from datasets import load_dataset\npubmed = load_dataset(\"ccdv/pubmed-summarization\")\nThe PubMed dataset contains article, abstract and section_names as columns. The first step after loading the dataset is tokenizing the training data. Tokenization is the process of splitting paragraphs, sentences into smaller units called tokens. tokenizer = AutoTokenizer.from_pretrained(‘facebook/bart-large-cnn’)\nThe next step is to preprocess the data. Before training the data, we need to convert our data into expected model input format.\ndef preprocess_function(examples):\n    inputs = [doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n\n    labels = tokenizer(examples[\"abstract\"], max_length=128, truncation=True, padding=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\nWe need to apply the processing function over the entire dataset. Setting flag batched=True helps to speed up the processing of multiple elements of the dataset at once.\ntokenized_pubmed = pubmed.map(preprocess_function, batched=True)\nNext, we need to create a batch for all the examples. Huggingface provides a data collator to create a batch for the examples.\ntokenized_datasets = tokenized_pubmed.remove_columns(pubmed[\"train\"].column_names)\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"facebook/bart-large-cnn\")\nHuggingface provides various pre-trained models that we can leverage to perform a variety of machine learning tasks.\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model)\nBefore training the model, we need to define our training hyperparamaters using training arguments. Since text summarization is a sequence to sequence tasks, we are using Seq2SeqTrainingArguments. And, we need to define our trainer by passing training and test dataset along with training arguments.\n# training arguments\ntraining_arguments = Seq2SeqTrainingArguments(\n            output_dir='./results',\n            evaluation_strategy='epoch',\n            learning_rate=2e-5,\n            per_device_train_batch_size=8,\n            per_device_eval_batch_size=8,\n            weight_decay=0.01,\n            save_total_limit=3,\n            num_train_epochs=3,\n            # remove_unused_columns=False,\n            # fp16=True,\n            )\n\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=tokenized_pubmed['train'],\n    eval_dataset=tokenized_pubmed['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n    )\nThe last step is to call train() to fine-tune our model.\ntrainer.train()\n\nConclusion\n\nSummarization helps to generalize the long documents by paraphrasing the important sentences from the whole document. It is very helpful in various applications like summarizing legal contracts, medical documents, news information and many more."
  },
  {
    "objectID": "posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html",
    "href": "posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html",
    "title": "Paper Explanation: Going deeper with Convolutions (GoogLeNet)",
    "section": "",
    "text": "Google proposed a deep Convolution Neural Network named inception that achieved top results for classification and detection in ILSVRC 2014.\n“Going deeper with convolutions” is actually inspired by an internet meme: ‘We need to go deeper’\nIn ILSVRC 2014, GoogLeNet used 12x fewer parameters than AlexNet used 2 years ago in 2012 competition."
  },
  {
    "objectID": "posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html#solution",
    "href": "posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html#solution",
    "title": "Paper Explanation: Going deeper with Convolutions (GoogLeNet)",
    "section": "Solution",
    "text": "Solution\nTo solve these issues, this paper comes up with the solution to form a ‘wider’ network rather than ‘deeper’ which is called as Inception module.\n\n\n\nThe ‘naive’ inception module performs convolutions on input from previous layer, with 3 different size of kernels or filters specifically 1x1, 3x3, and 5x5. In addition to this, max pooling is also performed. Outputs are then concatenated and sent to the next inception module.\nOne problem to the ‘naive’ approach is, even having 5x5 convolutions can lead to require large resource in terms of computations. This problem emerges more once pooling is added.\nTo make our networks inexpensive computationally, authors applied dimensionality reductions by adding 1x1 convolutions before 3x3 and 5x5 convolutions. Let’s see how these affect the number of parameters in the neural network.\nLet’s see what 5x5 convolution would be computationally\nComputation for above convolution operation is:\n(5²)(192)(32)(28²) = 120,422,400 operations\nTo bring down such a great number of operations, dimensionality reduction can be used. Here, it is done by convolving with 1x1 filters before performing convolution with bigger filters.\n\n\n\n5×5 Convolution with Dimensionality Reduction\nAfter dimensionality reduction number of operations for 5x5 convolution becomes:\n(1²)(192)(16)(28²) = 2,408,448 operations for the 1 × 1 convolution and,\n(5²)(16)(32)(28²) = 10,035,200 operations for the 5 × 5 convolution.\nIn total there will be 2,408,448 + 10,035,200 = 12,443,648 operations. There is large amount of reduction in computation.\nSo, after applying dimensionality reduction, our inception module becomes:\nGoogLeNet was built using inception module with dimensionality reduction. GoogLeNet consists of 22 layers deep network (27 with pooling layers included). All the convolutions, including the convolutions inside inception module , uses rectified linear activation.\n\n\n\nGoogLeNet incarnation of the Inception architecture. Source: Original Paper\n\nAll the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224x224 taking RGB color channels with mean sub-traction. “#3x3reduce” and “#5x5reduce” stands for the number of 1x1 filters in the reduction layer used before the 3x3 and 5x5 convolutions. One can see the number of 1x1 filters in the pro-jection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well\nOriginal Paper\n\nGoogLeNet is 22 layer deep counting only layers with parameters. With such deep network the may arise a problem such as vanishing gradient. To eliminate this, authors introduced auxiliary classifiers that are connected to intermediate layers, and helps the gradient signals to propagate back. These auxiliary classifiers are added on top of the output of Inception (4a) and (4d) modules. The loss from auxiliary classifiers are added during training and discarded during inference.\nThe exact structure of the extra network on the side, including the auxiliary classifier, is as follows:\n\nAn average pooling layer with 5x5 filter size and stride 3, resulting in an 4x4 512 output for the (4a), and 4x4 528 for the (4d) stage.\nA 1x1 convolution with 128 filters for dimension reduction and rectified linear activation.\nA fully connected layer with 1024 units and rectified linear activation.\nA dropout layer with 70% ratio of dropped outputs\nA linear layer with softmax loss as the classifier (predicting the same 1000 classes as the main classifier, but removed at inference time).\n\nThe systematic view of GoogLeNet architecture is shown below:\n\n\n\nGoogLeNet architecture\nGoogLeNet consists of a total of 9 inception modules namely 3a, 3b, 4a, 4b, 4c, 4d , 4e, 5a and 5b."
  },
  {
    "objectID": "posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html#googlenet-implementation",
    "href": "posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html#googlenet-implementation",
    "title": "Paper Explanation: Going deeper with Convolutions (GoogLeNet)",
    "section": "GoogLeNet implementation",
    "text": "GoogLeNet implementation\nHaving known about inception module and its inclusion in GoogLeNet architecture, we now implement GoogLeNet in tensorflow. This implementation of GoogLeNet is inspired from analytics vidya article on inception net.\nImporting the required libraries:\nfrom tensorflow.keras.layers import Layer\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Input, concatenate, GlobalAveragePooling2D, AveragePooling2D, Flatten\nimport cv2\nimport numpy as np\nfrom keras.utils import np_utils\nimport math\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nNext we are using cifar10 dataset as our data.\nnum_classes = 10\ndef load_cifar_data(img_rows, img_cols):\n  #Loading training and validation datasets\n  (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n  #Resizing images\n  X_train = np.array([cv2.resize(img, (img_rows, img_cols)) for img in X_train[:,:,:,:]])\n  X_valid = np.array([cv2.resize(img, (img_rows, img_cols)) for img in X_valid[:,:,:,:]])\n  #Transform targets to keras compatible format\n  Y_train = np_utils.to_categorical(Y_train, num_classes)\n  Y_valid = np_utils.to_categorical(Y_valid, num_classes)\n  X_train = X_train.astype('float32')\n  X_valid = X_valid.astype('float32')\n  #Preprocessing data\n  X_train = X_train / 255.0\n  Y_train = X_valid / 255.0\n  return X_train, Y_train, X_valid, Y_valid\n\nX_train, Y_trian, X_test, y_test = load_cifar_data(224,224)\nNext comes our inception module\nInception module contains 1x1 convolutions before 3x3 and 5x5 convolution operations. It takes different number of filters for different convolution operations and concatenate these operations to take into next layer.\ndef inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj, name=None):\n  conv_1x1 = Conv2D(filters_1x1, (1,1), activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n  conv_3x3 = Conv2D(filters_3x3_reduce, (1,1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n  conv_3x3 = Conv2D(filters_3x3, (3,3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n  conv_5x5 = Conv2D(filters_5x5_reduce, (1,1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n  conv_5x5 = Conv2D(filters_5x5, (3,3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n  pool_proj = MaxPool2D((3,3), strides=(1,1), padding='same')(x)\n  pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n  output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n  return output\n\nimport tensorflow\nkernel_init = tensorflow.keras.initializers.GlorotUniform()\nbias_init = tensorflow.initializers.Constant(value=0.2)\n\ninput_layer = Input(shape=(224, 224, 3))\nx = Conv2D(64, (7,7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\nx = MaxPool2D((3,3), padding='same', strides=(2,2), name='max_pool_1_3x3/2')(x)\nx = Conv2D(64, (1,1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\nx = Conv2D(192, (3,3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\nx = MaxPool2D((3,3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\nx = inception_module(x,\n                     filters_1x1=64,\n                     filters_3x3_reduce=96,\n                     filters_3x3=128,\n                     filters_5x5_reduce=16,\n                     filters_5x5=32,\n                     filters_pool_proj=32,\n                     name='inception_3a')\nx = inception_module(x,\n                     filters_1x1=128,\n                     filters_3x3_reduce=128,\n                     filters_3x3=192,\n                     filters_5x5_reduce=32,\n                     filters_5x5=96,\n                     filters_pool_proj=64,\n                     name='inception_3b')\nx = MaxPool2D((3,3), strides=(2, 2), padding='same', name='max_pool_3_3x3/2')(x)\nx = inception_module(x,\n                     filters_1x1=192,\n                     filters_3x3_reduce=96,\n                     filters_3x3=208,\n                     filters_5x5_reduce=16,\n                     filters_5x5=48,\n                     filters_pool_proj=64,\n                     name='inception_4a')\nx1 = AveragePooling2D((5,5), strides=3)(x)\nx1 = Conv2D(128, (1,1), padding='same', activation='relu')(x1)\nx1 = Flatten()(x1)\nx1 = Dense(1024, activation='relu')(x1)\nx1 = Dropout(0.4)(x1)\nx1 = Dense(10, activation='softmax', name='auxiliary_output_1')(x1)\nx = inception_module(x,\n                     filters_1x1=160,\n                     filters_3x3_reduce=112,\n                     filters_3x3=224,\n                     filters_5x5_reduce=24,\n                     filters_5x5=64,\n                     filters_pool_proj=64,\n                     name='inception_4b')\nx = inception_module(x,\n                     filters_1x1=128,\n                     filters_3x3_reduce=128,\n                     filters_3x3=256,\n                     filters_5x5_reduce=24,\n                     filters_5x5=64,\n                     filters_pool_proj=64,\n                     name='inception_4c')\nx = inception_module(x,\n                     filters_1x1=112,\n                     filters_3x3_reduce=144,\n                     filters_3x3=288,\n                     filters_5x5_reduce=32,\n                     filters_5x5=64,\n                     filters_pool_proj=64,\n                     name='inception_4d')\nx2 = AveragePooling2D((5,5), strides=3)(x)\nx2 = Conv2D(128, (1,1), padding='same', activation='relu')(x2)\nx2 = Flatten()(x2)\nx2 = Dense(1024, activation='relu')(x2)\nx2 = Dropout(0.4)(x2)\nx2 = Dense(10, activation='softmax', name='auxiliary_output_2')(x2)\nx = inception_module(x,\n                     filters_1x1=256,\n                     filters_3x3_reduce=160,\n                     filters_3x3=320,\n                     filters_5x5_reduce=32,\n                     filters_5x5=128,\n                     filters_pool_proj=128,\n                     name='inception_4e')\nx = MaxPool2D((3,3), strides=(2,2), padding='same', name='max_pool_4_3x3/2')\nx = inception_module(x,\n                     filters_1x1=256,\n                     filters_3x3_reduce=160,\n                     filters_3x3=320,\n                     filters_5x5_reduce=32,\n                     filters_5x5=128,\n                     filters_pool_proj=128,\n                     name='inception_5a')\nx = inception_module(x,\n                     filters_1x1=384,\n                     filters_3x3_reduce=192,\n                     filters_3x3=384,\n                     filters_5x5_reduce=48,\n                     filters_5x5=128,\n                     filters_pool_proj=128,\n                     name='inception_5b')\nx = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\nx = Dropout(0.4)(x)\nx = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(input_layer, [x, x1, x2], name='inception_v1')\nGetting the summary of the model\nmodel.summary()\nepochs = 25\ninitial_lrate = 0.01\ndef decay(epoch, steps=100):\n    initial_lrate = 0.01\n    drop = 0.96\n    epochs_drop = 8\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n    return lrate\nsgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\nlr_sc = LearningRateScheduler(decay, verbose=1)\nmodel.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])\nUsing our model to fit the training data\nhistory = model.fit(X_train, [y_train, y_train, y_train], validation_data=(X_test, [y_test, y_test, y_test]), epochs=epochs, batch_size=256, callbacks=[lr_sc])\nReferences\n\nhttps://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/\nGoing Deeper with Convolutions\nhttps://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"
  },
  {
    "objectID": "posts/2021-01-01-deep-residual-learning-for-image-recognition-resnet-paper-explained.html",
    "href": "posts/2021-01-01-deep-residual-learning-for-image-recognition-resnet-paper-explained.html",
    "title": "Deep Residual Learning for Image Recognition (ResNet paper explained)",
    "section": "",
    "text": "Deep Neural Networks tend to provide more accuracy as the number of layers increases. But, as we go more deeper in the network, the accuracy of the network decreases instead of increasing. As more layers are stacked, there occurs a problem of vanishing gradients. The paper mention that vanishing gradient has been addressed by normalized initialization and intermediate normalization layers. With the increase in depth, the accuracy gets saturated and then degrades rapidly.\n*Vanishing gradient: Vanishing gradient is a situation where a deep multilayer feedforward network or RNN is unable to propagate useful gradient information from output end of the model to the layers near the input end of the model. In this case, the gradient becomes very small and prevents weights from changing its value. It causes network hard to train.\n\n\n\n\n\nTraining error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer “plain” networks source\n\n\nThe above figure shows that with the increase in depth of the network, training error increases thus increasing test error. Here, the training error on 20 layer network is less than that of 56 layer network. Thus, the network cannot generalize well for new data and becomes an inefficient model. This degradation indicates that increasing the model layer does not aid in the performance of the model and not all the system are easy to optimize.\nThe paper address the degradation problem by introducing a deep residual learning framework. The main innovation for ResNet is the residual module. Residual module is specifically an identity residual module, which is a block of two convolutional layers with same number of filters and a small filter size. The output of the second layer is added with the input to the first convolution layer.\n\n\n\n\nResidual learning: a building block. source\n\n\nNetwork Architecture\nThe paper took baseline model of VGGNet as a plain network with mostly 3x3 filters with two design rules: a) for the same output feature map size, the layers have the same number of filters and b) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer. The network is ended with a global average pooling layer and a 1000-way fully connected layer with a softmax.\nBased on the plain network, shortcut connections are added to transform plain version into residual version.\n\n\n\n\nLeft: VGG-19 model Middle: Plain network with 34 parameter layers Right: residual network with 34 parameter layers source\n\n\n\nImplementation\n\nImage was first resized with its shorter side sampled into 256 x 480\nData augmentation techniques was carried out\nBatch normalization was carried out after each convolution and before activation\nStochastic gradient descent was used for training the network with mini batch of 256.\nWeight decay of 0.0001 and momentum of 0.9 was used.\n\n\n\nExperiments\nResnet architecture was evaluated on ImageNet 2012 classification dataset consisting of 1000 classes. The model was trained on the 1.28 million training images and evaluated on the 50k validation images. Moreover, 100k images were used for testing the model accuracy.\nWhile performing experiments on plain networks, a 34-layer plain network showed a higher validation error than an 18-layer plain network. Training error for the 34-layer plain network was found to be higher than the 18-layer plain network. Here, a degradation problem occurred as we go deep into the network. The deep plain networks may have a low convergence rate that impacts the accuracy of the model (impacts in reducing the training error).\nDifferent from the plain network, a shortcut connection was added to each pair of 3x3 filters. With a same number of layers as in plain network, Resnet 34 performed better than Resnet 18 network. Resnet-34 showed less error and performs well in generalizing validation data. This resolves the problem of degradation as seen on a plain deep network. The comparison for both plain and residual network is shown below:\n\n\n\n\n\nTraining on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. source\n\n\n\nReferences\n\nDeep Residual Learning for Image Recognition"
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html",
    "title": "MobileNet Architecture Explained",
    "section": "",
    "text": "In this blog post, I will try to write about the MobileNets and its architecture. MobileNet uses depthwise separable convolutions instead of standard convolution to reduce model size and computation. Hence, it can be used to build light weight deep neural networks for mobile and embedded vision applications."
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html#topics-covered",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html#topics-covered",
    "title": "MobileNet Architecture Explained",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nStandard convolutions and depthwise separable convolutions\nMobileNet Architecture\nWidth Multiplier to achieve thinner models\nResolution Multiplier for reduced representation\nArchitecture Implementation"
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html#standard-convolutions-and-depthwise-separable-convolutions",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html#standard-convolutions-and-depthwise-separable-convolutions",
    "title": "MobileNet Architecture Explained",
    "section": "Standard convolutions and depthwise separable convolutions",
    "text": "Standard convolutions and depthwise separable convolutions\nConvolution operation consists of an input image, a kernel or filter that slides through the input image and outputs a feature map. The main aim of convolution operation is to extract features from the input image. As we know, every image can be considered as a matrix of pixel values. Consider an input as 5x5 matrix with values of pixels 0 and 1 as shown below:\n\n\n\nAlso, consider another 3x3 matrix as below:\n\n\n\nThe convolution operation for input size 5x5 with filter of 3x3 is shown below:\n\n\n\n\nFig: The Convolution operation. source\n\nWe are sliding the 3x3 matrix over 5x5 input matrix and performing element-wise matrix multiplication and adding the multiplication output to get convolved feature. The output obtained from such operation is also called as feature map. The 3x3 matrix that is sliding over the input matrix is known as filter or kernel. More on convolution can be found at this amazing article.\n\nSeparable Convolutions\nBefore knowing what depth-wise separable convolutions do, let’s know about separable convolutions. There are two types of separable convolutions: spatial separable convolutions and depthwise separable convolutions.\n\n\nSpatial separable convolutions\nSpatial Separable convolutions deals with spatial dimension of the image (width and height). It divides a kernel into two smaller kernel. For example, a \\(3\\*3\\) kernel is divided into a \\(3\\*1\\) and a \\(1\\*3\\) kernel.\n\n\n\nHere, instead of doing one convolution with 9 multiplicants, we can do two convolutions with 3 multiplications each (i.e., 6 in total) to achieve the same effect. With less multiplications, computational complexity goes down and network is able is run faster.\nOne of the famous convolution used to detect edges i.e., Sobel kernel can also be separated spatially.\n\n\n\nThough, less computation power is achieved using spatial separable convolution, all the kernels cannot be separated into two smaller kernels, which is one of the cons of spatial separable convolution.\n\n\nDepthwise Separable Convolutions\nDepthwise Separable Convolutions is what Mobilenet architecture is based on. Depthwise separable convolution works with kernel that cannot be factored into two smaller kernels. Spatial separable convolutions deals with spatial dimensions but depthwise separable convolutions deals with depth dimension also.\nDepthwise separable convolution is a factorized convolution that factorizes standard convolution into a depthwise convolution and a \\(1*1\\) convolution called pointwise convolution. Depthwise separable convolutions splits kernel into two separate kernels for filtering and combining. Depthwise convolution is used for filterning whereas pointwise convolution is used for combining.\nUsing depthwise separable convolutions, the total computation required for the operation is the sum of depthwise convolution and pointwise convolution which is:\n\n\n\nFor standard convolution, total computation is:\n\\(D_K . D_K . M . N . D_F . D_F\\), where computational cost depends on number of input channels \\(M\\), number of output channels \\(N\\), kernel size \\(D_K\\) and feature map size \\(D_F\\).\nBy expressing convolution as a two steps process of filtering and combining, total reduction in computation is:\n\\(\\frac{D_K . D_K . M . D_F . D_F + M . N . D_F . D_F}{D_K . D_K . M. N. D_F . D_F}\\), which is equivalent to \\(\\frac{1}{N} + \\frac{1}{D_k^2}\\)\nThat means when \\(D_K * D_K\\) is 3*3, computation cost can be reduced to 8 to 9 times.\nMore on depthwise separable convolutions can be found here."
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html#mobilenet-architecture",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html#mobilenet-architecture",
    "title": "MobileNet Architecture Explained",
    "section": "MobileNet Architecture",
    "text": "MobileNet Architecture\n\nAs mentioned above, mobilenet is built on depthwise separable convolutions, except for first layer. First layer is a full convolutional layer.\nAll layers are followed by batch normalization and ReLU non-linearity. However, final layer is a fully connected layer without any non-linearity and feeds to the softmax for classification.\nFor down sampling, strided convolution is used for both depthwise convolution as well as for first fully convolutional layer.\nThe total number of layers for mobilenet is 28 considering depthwise and pointwise convolution as separate layers.\n\n\n\n\n\nfig. (left) Standard convolution with batchnorm and relu\n(right) depthwise and pointwise convolution followed by batchnorm and relu source\n\nMobileNet architecture is shown below:\n\n\n\n\nFig. MobileNet architecture"
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html#width-multiplier-to-achieve-thinner-models",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html#width-multiplier-to-achieve-thinner-models",
    "title": "MobileNet Architecture Explained",
    "section": "Width Multiplier to achieve Thinner Models",
    "text": "Width Multiplier to achieve Thinner Models\nThough our mobilenet model is smaller and computationally less expensive, sometimes we need our model to be more smaller and less expensive in terms of computation. To construct these models, a separate parameter \\(\\alpha\\) is used called as width multiplier. Width multiplier helps to make network thinner uniformly at each layer. For any given layer and width multiplier \\(\\alpha\\), the number of input channels \\(M\\) becomes \\(\\alpha M\\) and the number of output channel \\(N\\) becomes \\(\\alpha N\\). Then, the computational cost for depthwise separable convolution with width multiplier becomes:\n\\(D_K . D_K . \\alpha M . D_F . D_F + \\alpha M . \\alpha N . D_F . D_F\\)\n\nWidth multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch. MobileNet paper"
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html#resolution-multiplier-for-reduced-representation",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html#resolution-multiplier-for-reduced-representation",
    "title": "MobileNet Architecture Explained",
    "section": "Resolution Multiplier for reduced representation",
    "text": "Resolution Multiplier for reduced representation\nResolution Multiplier is another parameter for reducing model computational cost. It is represented by \\(\\rho\\). It is applied to the input image and internal representation of every layer is reduced by the same multiplier. The computational cost for depthwise separable convolution with width multiplier becomes:\n\\(D_K . D_K . \\alpha M . \\rho D\\_F . \\rho D_F + \\alpha M . \\alpha N . \\rho D_F . \\rho D_F\\)\nThe value of \\(\\rho\\) = 1 is the base mobilenet and \\(\\rho<1\\) is the reduced computational MobileNets."
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html#architecture-implementation",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html#architecture-implementation",
    "title": "MobileNet Architecture Explained",
    "section": "Architecture Implementation",
    "text": "Architecture Implementation\nMobileNet uses depthwise separable convolutions where each layers is followed by BatchNormalization and ReLU non-linearity. MobileNet contains a depthwise and a pointwise convolution layer. The code snippets inspired from MLT.\n# First we will build mobilenet block\ndef mobilenet_block(x, filters, strides):\n  x = keras.layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding='same')(x)\n  x = keras.layers.BatchNormalization()(x)\n  x = keras.layers.ReLU()(x)\n\n  x = keras.layers.Conv2D(filters=filters, kernel_size=1, strides=1, padding='same')(x)\n  x = keras.layers.BatchNormalization()(x)\n  x = keras.layers.ReLU()(x)\n  return x\nMobileNet uses input_shape of 224*224*3. First layer of mobilenet is a Convolutional layer with 32 filters, 3*3 kernel and stride of 2. This is followed by BatchNormalization and ReLU non-linearity.\nINPUT_SHAPE = 28, 28, 3\ninput = keras.layers.Input(INPUT_SHAPE)\nx = keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.ReLU()(x)\nAfter first layers, there is a series of mobilenet block with different kernel sizes and filters.\nx = mobilenet_block(x, filters=64, strides=1)\n\nx = mobilenet_block(x, filters=128, strides=2)\nx = mobilenet_block(x, filters=128, strides=1)\n\nx = mobilenet_block(x, filters=256, strides=2)\nx = mobilenet_block(x, filters=256, strides=1)\n\nx = mobilenet_block(x, filters=512, strides=2)\nfor _ in range(5):\n  x = mobilenet_block(x, filters=512, strides=1)\n  x = keras.layers.AveragePooling2D(pool_size=7, strides=1)(x)\n  output = keras.layers.Dense(1000, activation='softmax')(x)"
  },
  {
    "objectID": "posts/2020-09-21-mobilenet-architecture-explained.html#references",
    "href": "posts/2020-09-21-mobilenet-architecture-explained.html#references",
    "title": "MobileNet Architecture Explained",
    "section": "References",
    "text": "References\n\nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\nMachine Learning Tokyo\nA Basic Introduction to Separable Convolutions\nAn Intuitive Explanation of Convolutional Neural Networks"
  },
  {
    "objectID": "posts/2020-08-23-neural-style-transfer-and-its-working.html",
    "href": "posts/2020-08-23-neural-style-transfer-and-its-working.html",
    "title": "Neural style transfer and its working",
    "section": "",
    "text": "Have you ever used an app called Prisma that styles your image using popular paintings and turns your photo stunning? If that’s the case then, the app you are using is the result of style transfer; a computer vision technique that combines your images with artistic style."
  },
  {
    "objectID": "posts/2020-08-23-neural-style-transfer-and-its-working.html#introduction",
    "href": "posts/2020-08-23-neural-style-transfer-and-its-working.html#introduction",
    "title": "Neural style transfer and its working",
    "section": "Introduction",
    "text": "Introduction\nStyle transfer is a computer vision technique that takes two images: content image and style image, combines them to form a resulting image that style the image based on style image taking contents from the content image.\nHere is how it looks like:\n\n\n\nThe content image you can see above is Van Gogh’s Starry Night Painting and the style image a image from Tubingen university from Germany. Resultant image is shown on the right side that used content of content image and is styled using style image.\n\nNow let’s get into the working of neural style transfer\nNeural Style Transfer is based on Deep Neural Network that create images of high perpetual quality. It uses neural network to separate and recombine content and style of images that we feed to obtain the desired result. The original paper uses 19 layer VGG network comprising of 16 convolutional layers, 5 max-pooling layers and 3 fully connected layers.\n\n\n\n\nFig. A 19 layer VGG network source"
  },
  {
    "objectID": "posts/2020-08-23-neural-style-transfer-and-its-working.html#how-exactly-do-we-obtain-such-images",
    "href": "posts/2020-08-23-neural-style-transfer-and-its-working.html#how-exactly-do-we-obtain-such-images",
    "title": "Neural style transfer and its working",
    "section": "How exactly do we obtain such images?",
    "text": "How exactly do we obtain such images?\nOur goal here is to apply style over our content image. We are not training any neural network in this case, rather we start from a blank image and optimize the cost function by changing the pixel values of the image. The cost function contains two losses: Content loss and Style loss.\nConsidering c as the content image and x as the style transferred image, content loss tends to \\(0\\) when \\(x\\) and \\(c\\) are close to each other and increases when these value gets increased.\nGiven the original image \\(vec{p}\\) and generated image \\(vec{x}\\), we can define the loss generated by the content image as:\n\n\n\nContent loss takes content weight which is a scalar that gives weighting for the content loss, content_current that gives features of the current image. content_current is the Pytorch tensor having shape (1, \\(C_l\\), \\(H_l\\), \\(W_l\\)), where\n\\(C_l\\) is the number of channels in layer \\(l\\), \\(H_l\\) and \\(W_l\\) are width and height."
  },
  {
    "objectID": "posts/2020-05-09-vggnet-architecture-explained.html",
    "href": "posts/2020-05-09-vggnet-architecture-explained.html",
    "title": "VGGNet Architecture Explained",
    "section": "",
    "text": "VGGNet is a Convolutional Neural Network architecture proposed by Karen Simonyan and Andrew Zisserman of University of Oxford in 2014. This paper mailny focuses in the effect of the convolutional neural network depth on its accuracy. You can find the original paper of VGGNet which is titled as Very Deep Convolutional Networks for Large Scale Image Recognition.\nArchitecture\nThe input to VGG based convNet is a 224*224 RGB image. Preprocessing layer takes the RGB image with pixel values in the range of 0 - 255 and subtracts the mean image values which is calculated over the entire ImageNet training set.\n\n\n\n\nFig. A visualization of the VGG architecture (source)\n\nThe input images after preprocessing are passed through these weight layers. The training images are passed through a stack of convolution layers. There are total of 13 convolutional layers and 3 fully connected layers in VGG16 architecture. VGG has smaller filters (3*3) with more depth instead of having large filters. It has ended up having the same effective receptive field as if you only have one 7 x 7 convolutional layers.\nAnother variation of VGGNet has 19 weight layers consisting of 16 convolutional layers with 3 fully connected layers and same 5 pooling layers. In both variation of VGGNet there consists of two Fully Connected layers with 4096 channels each which is followed by another fully connected layer with 1000 channels to predict 1000 labels. Last fully connected layer uses softmax layer for classification purpose.\n\n\n\n\n\nVGG 16 and VGG 19 Layers ( source)\n\n\nArchitecture walkthrough:\n\nThe first two layers are convolutional layers with 3x3 filters, and first two layers use 64 filters that results in 224x224x64 volume as same convolutions are used. The filters are always 3x3 with stride of 1\nAfter this, pooling layer was used with max-pool of 2x2 size and stride 2 which reduces height and width of a volume from 224x224x64 to 112x112x64.\nThis is followed by 2 more convolution layers with 128 filters. This results in the new dimension of 112x112x128.\nAfter pooling layer is used, volume is reduced to 56x56x128.\nTwo more convolution layers are added with 256 filters each followed by down sampling layer that reduces the size to 28x28x256.\nTwo more stack each with 3 convolution layer is separated by a max-pool layer.\nAfter the final pooling layer, 7x7x512 volume is flattened into Fully Connected (FC) layer with 4096 channels and softmax output of 1000 classes.\n\nImplementation\nNow let’s go ahead and see how we can implement this architecture using tensorflow. This implementation is inspired from Machine Learning Tokyo’s CNN architectures.\nImporting libraries\nStarting the convolution blocks with the input layer\ninput = Input(shape=(224,224,3))\n1st block consists of 2 convolution layer each with 64 filters of 3*3 and followed by a max-pool layer with stride 2 and pool-size of 2. All hidden layer uses ReLU for non-linearity.\nx = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(input)\nx = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n2nd block also consists of 2 convolution layer each with 128 filters of 3*3 and followed by a max-pool layer with stride 2 and pool-size of 2.\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n3rd block consists of 3 convolution layer each with 256 filters of 3*3 and followed by a max-pool layer with stride 2 and pool-size of 2.\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n4th and 5th block consists of 3 convolutional layers with 512 filters each. In between these blocks, a max-pool layer is used with stride of 2 and pool-size of 2.\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\nThe output from 5th convolution block is Flattened which gives 4096 units. This fully connected layer is connected to another FC layer having same number of units. The final fully connected layer contains 1000 units and softmax activation which is used for classification of 1000 classes\n#Dense Layers\nx = Flatten(x)\nx = Dense(units=4096, activation='relu')(x)\nx = Dense(units=4096, activation='relu')(x)\noutput = Dense(units=1000, activation='softmax')(x)\nfrom tensorflow.keras import Model\nmodel = Model(inputs=input, outputs=output)"
  },
  {
    "objectID": "posts/2021-03-26-simclr-explained.html",
    "href": "posts/2021-03-26-simclr-explained.html",
    "title": "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)",
    "section": "",
    "text": "Various self-supervised learning methods have been proposed in recent years for learning image representations. Though a lot of methods have been proposed, the performance of those methods was found less effective in terms of accuracy than those of supervised counterparts. But SimCLR has provided promising results, thus taking self-supervised learning to a new level. It uses a contrastive learning approach. This paper introduces a simple framework to learn representations from unlabeled images based on heavy data augmentation. Before going deep into simCLR and its details, let’s see what contrastive learning is:"
  },
  {
    "objectID": "posts/2021-03-26-simclr-explained.html#a-simple-framework-for-contrastive-learning-of-visual-representations---simclr",
    "href": "posts/2021-03-26-simclr-explained.html#a-simple-framework-for-contrastive-learning-of-visual-representations---simclr",
    "title": "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)",
    "section": "A Simple Framework for Contrastive Learning of Visual Representations - SimCLR",
    "text": "A Simple Framework for Contrastive Learning of Visual Representations - SimCLR\nHow does simCLR learn representations?\nsimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss.\nInorder to learn good contrastive representation learning, simCLR consists of four major components\n-Data Augmentation module: Data augmentation is more beneficial for unsupervised contrastive learning than supervised learning. The data augmentation module transforms any given data example into two correlated views of the same example. These examples are denoted as \\(\\\\widetilde{x\\_i}\\) and \\(\\\\widetilde{x\\_j}\\), considered as positive pair. The authors mainly applied three augmentations sequentially: random cropping followed by resizing to the original size, random color distortions, and random Gaussian blur.\n\nEncoder: A neural base encoder \\(f(.)\\) is used that extracts features from augmented data examples. ResNet is used as the architecture to extract those representations. The learned representation is the result of the average pooling layer.\nProjection head: The projection head \\(g(.)\\) is a MLP with one hidden layer that maps representations from the base encoder network to space where contrastive loss is applied. Here ReLU activation function is used for non-linearity.\nContrastive loss function: For any given set of \\(\\\\widetilde{x\\_k}\\) which includes positive example pair \\(\\\\widetilde{x\\_i}\\) and \\(\\\\widetilde{x\\_j}\\), contrastive prediction task aims to identify \\(\\\\widetilde{x\\_j}\\) in {\\(\\\\widetilde{x\\_k}\\)} (here i and k are not equal) for given \\(\\\\widetilde{x\\_i}\\)\n\n\n\n\n\nsimCLR Framework Google AI"
  },
  {
    "objectID": "posts/2021-03-26-simclr-explained.html#working-of-simclr-algorithm",
    "href": "posts/2021-03-26-simclr-explained.html#working-of-simclr-algorithm",
    "title": "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)",
    "section": "Working of simCLR algorithm",
    "text": "Working of simCLR algorithm\nFirst, we generate a batch of N examples and define contrastive prediction tasks on augmented examples. After applying a series of data augmentation techniques random(crop + resize + color distortion + grayscale) on N examples, 2N data points are generated (since we are generating similar pairs in a batch). Each augmented image is passed in a pair through the base encoder to get a representation from the image. Followed by the base encoder, a projection head is used that maps the base encoder to the representation \\(z\\_i\\) and \\(z\\_j\\) as presented in the paper. For each augmented image, we get embedding vectors for it. These embedding vectors are later subjected for calculating loss.\n\n\n\n\nsimCLR algorithm"
  },
  {
    "objectID": "posts/2021-03-26-simclr-explained.html#calculating-loss",
    "href": "posts/2021-03-26-simclr-explained.html#calculating-loss",
    "title": "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)",
    "section": "Calculating loss",
    "text": "Calculating loss\nAfter getting the representations of the augmented images, the similarity of those images is calculated using cosine similarity. For two augmented image, \\(x\\_i\\) and \\(x\\_j\\), cosine similarity is calculated on projected representations \\(z\\_i\\) and \\(z\\_j\\).\n\\(s\\_{i,j} = \\\\frac{z\\_i^{T}z\\_j}{||z\\_i||||z\\_j||}\\), where\n\\(T\\) denotes a temperature parameter,\n\\(||z||\\) is the norm of the vector\nsimCLR uses NT-Xent (Normalized temperature-scaled cross entropy loss) for calculating the loss.\n\n\n\nHere \\(z\\_i\\) and \\(z\\_j\\) are the output vectors obtained from the projection head\nAfter training simCLR on the contrastive learning task, it can be used for transfer learning. For downstream tasks, representations from encoder are used rather than the representation from projection head. These representations can be used for tasks such as classification, detection."
  },
  {
    "objectID": "posts/2021-03-26-simclr-explained.html#results",
    "href": "posts/2021-03-26-simclr-explained.html#results",
    "title": "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)",
    "section": "Results",
    "text": "Results\nThe proposed simCLR outperformed previous self-supervised and semi-supervised methods on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100× fewer labels.\n\n\n\n\nImageNet Top-1 accuracy of linear classifiers trained\non representations learned with different self-supervised methods (pre-trained on ImageNet). Gray cross indicates supervised\nResNet-50. Our method, SimCLR, is shown in bold"
  },
  {
    "objectID": "posts/2021-03-26-simclr-explained.html#references",
    "href": "posts/2021-03-26-simclr-explained.html#references",
    "title": "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)",
    "section": "References",
    "text": "References\n\n“A Simple Framework for Contrastive Learning of Visual Representations”\n“SimCLR Slides, Google Brain Team”\nContrastive Representation Learning: A Framework and Review\nThe Illustrated SimCLR Framework"
  },
  {
    "objectID": "posts/2023-05-15-gpt4-summary.html",
    "href": "posts/2023-05-15-gpt4-summary.html",
    "title": "Brief overview of GPT-4",
    "section": "",
    "text": "Since the release of ChatGPT, there has been significant interest and discussion within the broader AI and natural language processing communities regarding its capabilities. In addition to this, ChatGPT has captured the attention of the internet at large due to its remarkable ability to generate fluent and natural-sounding responses across a wide range of prompts and language tasks. Due to this, it became fastest growing consumer application in the history, just two months after the launch. ChatGPT is fine-tuned from a model in the GPT-3.5 series and can write articles, jokes, poetrys in response to the prompt. Though powerful, there have also been concerns raised about the potential risks associated with it and other large language models (LLMs), particularly with respect to issues such as bias, and misinformation. One of the major concern for LLMs is that it suffers from hallucination.\nA year after releasing ChatGPT, OpenAI released GPT-4 (on 14th March, an improved version of GPT-3.5 model that supports multimodal data. It is capable of processing text and image data to generate textual data. It achieved human level performance on various professional and academic benchmarks. On a simulated bar exam, GPT-4 achieved a score that falls on the top 10% of the exam takes. In contrast, the score achieved by previous model GPT-3.5 fell on bottom 10%. This shows the level of improvement achieved by the latest version of GPT. It is also important to mention that the model was not specifically trained on these exams. A minority of problems were seen by model while training."
  },
  {
    "objectID": "posts/2023-05-15-gpt4-summary.html#capabilities-of-gpt-4",
    "href": "posts/2023-05-15-gpt4-summary.html#capabilities-of-gpt-4",
    "title": "Brief overview of GPT-4",
    "section": "Capabilities of GPT-4",
    "text": "Capabilities of GPT-4\nThough the report does not provide any details about architecture (including model size), hardware, training compute, dataset construction, or training method, a demo run by Greg Brockman (President and Co-founder, OpenAI) after the release of GPT-4 shows various capabilities of the model.\nYou can watch the GPT-4 Developer Livestream replay here:\n\n\n1. Supports longer context\nGPT-4 is capable of handling over 25,000 words of text, that enables its usage in situations that require the creation of lengthy content, extended dialogues, or the exploration and analysis of extensive documents.\n\n\n2. Hand-drawn pencil drawing turned into a fully functional website\nGPT-4 is also capable of handling visual input, such as hand-drawn pencil drawings that looks like a mock design, and generating code to create a website. The generated output is mind blowing. Another important aspect is the accuracy by which the model is able to perform OCR task with such messy handwritings.\n\n\nFig. Left is the mock design and right is the website created using the code generated from gpt4-model. source\n\n\n\n3. GPT-4 can describe the image.\nAs opposed to text on prompts (on previous GPT version), this model accepts inputs containing both text and images. It lets user specify any language or vision tasks. GPT-4 displays comparable skills on various types of content, such as documents containing both textual and visual elements like photographs, diagrams, or screenshots, as it does when dealing with text-only inputs.\n\n\n\n\n\n\nExample prompt demonstrating GPT-4’s visual input capability. The prompt consists of a question about an image with multiple panels which GPT-4 is able to answer. source\n\n\n\n4. Human level performance on professional and academic benchmarks\nGPT outperforms the previous state-of-the-art models on various standardized exams, such as GRE, SAT, BAR, and APs, along with other research benchmarks like MMLU, HellaSWAG, and TextQA. GPT-4 outperforms the English language performance of GPT 3.5 and existing language models (Chinchilla and PaLM), including low-resource languages such as Latvian, Welsh, and Swahili."
  },
  {
    "objectID": "posts/2023-05-15-gpt4-summary.html#limitations-of-gpt-4",
    "href": "posts/2023-05-15-gpt4-summary.html#limitations-of-gpt-4",
    "title": "Brief overview of GPT-4",
    "section": "Limitations of GPT-4",
    "text": "Limitations of GPT-4\n\nThough there has been a tremendous improvement as compared to previous models, GPT-4 has similar limitations as earlier GPT models. It is not fully reliable and hallucinates.\nSince GPT-4 is trained on the data available till September 2021, it lacks knowledge of the events occured after that time period."
  },
  {
    "objectID": "posts/2023-05-15-gpt4-summary.html#risks-and-mitigations",
    "href": "posts/2023-05-15-gpt4-summary.html#risks-and-mitigations",
    "title": "Brief overview of GPT-4",
    "section": "Risks and mitigations",
    "text": "Risks and mitigations\nThe prompts entered by the users are not always safe. When providing unsafe inputs to the model, it may generate undesirable text like commiting crimes. To mitigate these risks, various approaches like Adversarial Testing, Model Assisted Safety Pipeline are carried out. Using domain experts and their findings, model is improved to refuse request for unsafe inputs like synthesizing dangerous chemicals.\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n                   Examples of how unsafe inputs are refused by the model"
  },
  {
    "objectID": "posts/2023-05-15-gpt4-summary.html#conclusion",
    "href": "posts/2023-05-15-gpt4-summary.html#conclusion",
    "title": "Brief overview of GPT-4",
    "section": "Conclusion",
    "text": "Conclusion\nThe recent advancements in GPT-4, have proven to outperform existing language models in a collection of NLP tasks. The improved capabilities of GPT-4 are not limited to the English language, as predictable scaling allows for accurate predictions in many different languages. However, the increased capabilities of GPT-4 also present new risks, which require significant work to understand and improve its safety and alignment. Nevertheless, GPT-4 marks a significant milestone towards the development of broadly useful and safely deployed AI systems."
  },
  {
    "objectID": "posts/2023-05-15-gpt4-summary.html#references",
    "href": "posts/2023-05-15-gpt4-summary.html#references",
    "title": "Brief overview of GPT-4",
    "section": "References:",
    "text": "References:\n\nGPT-4 Technical Report\nGPT-4 Blog Post\nchat.openai.com"
  },
  {
    "objectID": "posts/2023-05-15-gpt4-summary.html#ps",
    "href": "posts/2023-05-15-gpt4-summary.html#ps",
    "title": "Brief overview of GPT-4",
    "section": "PS",
    "text": "PS\nWhile GPT-4 may have stolen the headlines, it was not the only new technology on display. AnthropicAI unveiled Claude, next gen AI assistant can help with use cases including summarization, search, creative and collaborative writing, Q&A, coding, and more. Meanwhile, Google AI released PaLM, an entry point for Google’s large language models with variety of applications. With these three new systems, the future of AI looks brighter than ever before."
  },
  {
    "objectID": "posts/2020-12-08-self-supervised-learning.html",
    "href": "posts/2020-12-08-self-supervised-learning.html",
    "title": "Self-supervised Learning",
    "section": "",
    "text": "I have been exploring self-supervised learning and been through papers and blogs to understand it. Self-supervised learning is considered the next big thing in deep learning and why not! If there is a way to learn without providing labels, then this enables us to leverage a large amount of unlabeled data for our tasks. I am going to provide my understanding of self-supervised learning and will try to explain some papers about it.\nWe have been familiar with supervised learning wherein we provide features and labels to train a model and the model uses those labels to learn from the features. But labeling data is not an easy task as it requires more time and manpower. There is a large amount of data being generated daily and is unlabeled. The generated data may be in the form of text, images, audio, or videos. Those data can be used for different purposes. But there is a catch. These data do not contain labels and it difficult to work on these sorts of data. Here comes self-supervised learning to the rescue."
  },
  {
    "objectID": "posts/2020-12-08-self-supervised-learning.html#self-supervised-techniques-for-images",
    "href": "posts/2020-12-08-self-supervised-learning.html#self-supervised-techniques-for-images",
    "title": "Self-supervised Learning",
    "section": "Self-supervised Techniques for Images",
    "text": "Self-supervised Techniques for Images\nMany ideas have been proposed for self-supervised learning on images. A more common methodology or workflow is to train a model in one or multiple pretext tasks with the use of unlabeled data and use that model to perform downstream tasks. Some of the proposed ideas of self-supervised techniques for images are summarized below:\n\nRotation\nTo learn representation by predicting image rotations, Gidaris et al. proposed an architecture where features are learned by training Convolution Nets to recognize rotations that are applied to the image before feeding to the network. This set of geometric transformations defines the classification pretext task that the model has to learn which can later be used for downstream tasks. Geometric transformation is made such that the image is rotated through 4 different angles (0, 90, 270, and 360). This way, our model has to predict one of the 4 transformations that are done on the image. To predict the task, our model has to understand the concept of objects such as their location, their type, and their pose.\n\n\n\n\nIllustration of the self-supervised task proposed for semantic feature learning.\nGiven four possible geometric transformations, the 0, 90, 180, and 270 degrees rotations,\na ConvNet model was trained to recognize the rotation that is applied to the image that it gets as input. (source)\n\nMore details at: Unsupervised Representation Learning By Predicting Image Rotations\n\n\nExemplar\nIn Exemplar-CNN ( Dosovitskiy et al., 2015 ), a network is trained to discriminate between a set of surrogate classes. Each surrogate class is formed by applying random data augmentations such as translation, scaling, rotation, contrast and color shifts. While creating surrogate training data:\n\nN patches of size 32 x 32 pixels are randomly sampled from different images at varying positions. Since, we are interested in patches objects or parts of objects, random patches are sampled only from region containing considerable gradients.\nEach patch is applied with a variety of image transformations. All the resulting transformed patches are considered to be in same surrogate classes.\n\nThe pretext task is to discriminate between the set of surrogate class.\n\n\n\n\nSeveral random transformations applied to one of the\npatches extracted from the STL unlabeled dataset. The original\npatch is in the top left corner ( source)\n\nMore details at: Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\n\n\nJigsaw Puzzle\nAnother approach of learning visual representation from unlabeled dataset is by training a ConvNet model to solve Jigsaw puzzle as a pretext task which can be later used for downstream tasks. In Jigsaw puzzle task, model is trained to place 9 shuffled patches back to the original position. To place shuffled patches to original position, Noroozi et al. proposed a Context Free Network (CFN) which is a siamese CNN that uses shared weights. The patches are combined in a fully connected layer.\n\n\n\n\nLearning image representations by solving Jigsaw puzzles.\n\n\n\nThe image from which the tiles (marked with green lines) are extracted. \nA puzzle obtained by shuffling the tiles. \nDetermining the relative position between the central tile and the top two tiles from the left can be very challenging\nsource\n\n\n\nFrom the set of defined puzzle permutations, one permutation is randomly picked to arrange those 9 patches as per that permutation. This results CFN to return a vector with a probability value for each index. Given those 9 tiles, there will be 9! = 362,880 possible permutations. This creates difficulty in jigsaw puzzles. To control this, the paper proposed to shuffle patches according to a predefined set of permutations and configured the model to predict a probability vector over all the indices in the set.\n\n\n\n\nContext Free Network Architecture (source)\n\nMore details at: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n\n\nRelative Patch Location\nThis approach by Doersch et al, predicts position of second patch of the image that is relative to the first patch. For this pretext task, a network is fed with two input patches and is passed through several convolutional layers. The network produces an output with probability to each of eight image patches. This can be taken as a classification problem with 8 classes where the input patch is assigned to one of these 8 classes to be considered as relative patch to the input patch.\n\n\n\n\nThe algorithm receives two patches in one of these eight\npossible spatial arrangements, without any context, and must then\nclassify which configuration was sampled (source)\n\nMore details at: Unsupervised Visual Representation Learning by Context Prediction\nReferences:\n\nThe Illustrated Self-Supervised Learning\nSelf-supervised Learning\nSelf-supervised Visual Feature Learning with Deep Neural Networks: A Survey"
  },
  {
    "objectID": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html",
    "href": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html",
    "title": "Deep Convolutional Generative Adversarial Networks (DCGANs)",
    "section": "",
    "text": "DCGAN (Deep Convolutional General Adversarial Networks) uses convolutional layers in its design."
  },
  {
    "objectID": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#architectural-details-for-dcgan",
    "href": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#architectural-details-for-dcgan",
    "title": "Deep Convolutional Generative Adversarial Networks (DCGANs)",
    "section": "Architectural Details for DCGAN",
    "text": "Architectural Details for DCGAN\n\nComprised convolutional network without max-pooling. Instead, it uses convolutional stride and transpose convolution for downsampling and upsampling respectively. To find out how pooling and convolutional stride differs please go through this.\nRemoved all fully connected layers\nUsed batch normalization to bring stability in learning. It is done by normalizing the input to have zero mean and a variance of one. Batchnormalization was added to all the layers except generator output layer and discriminator input layer\nReLU activation is used in the generator except for the output layer which uses tanh activation function\nLeakyReLU activation is used at all layers in the discriminator"
  },
  {
    "objectID": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#training-generator-in-dcgan",
    "href": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#training-generator-in-dcgan",
    "title": "Deep Convolutional Generative Adversarial Networks (DCGANs)",
    "section": "Training Generator in DCGAN",
    "text": "Training Generator in DCGAN\n[latexpage]\nGenerator takes a uniform noise distribution \\(z\\) as input. This input is reshaped with the help of fully connected layer into three dimensional layer with small base (width * height) and depth. Then, using transposed convolution, the output from previous layer is upsampled. Each transoposed convolution layer is followed by batch normalization to normalize the input. This helps in stabilizing the training of our GAN.\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#details-of-adversarial-training",
    "href": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#details-of-adversarial-training",
    "title": "Deep Convolutional Generative Adversarial Networks (DCGANs)",
    "section": "Details of Adversarial Training",
    "text": "Details of Adversarial Training\nDCGAN was trained on three datasets: Large-scale Scene Understanding (LSUN), ImageNet-1k and Faces dataset.\n\nTraining images were scaled to the range of tanh activation function [-1, 1] and no further pre-processing was performed\nAll models were trained with mini-batch size of 128\nWeights were initialized from normal normal distribution with mean 0 and standard deviation of 0.2\nIncase of LeakyReLU, the value of alpha was set to 0.2\nAdam optimizer was used for updating weights. Learning rate was set to 0.001 and momentum term was reduced to 0.5 from 0.9"
  },
  {
    "objectID": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#dataset-details",
    "href": "posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html#dataset-details",
    "title": "Deep Convolutional Generative Adversarial Networks (DCGANs)",
    "section": "Dataset Details",
    "text": "Dataset Details\n\nDCGAN model was trained on LSUN bedroom dataset comprising over 3 million training images.\nNo data augmentation was used\nDe-duplication process was performed to decrease the likelihood of generator memorizing input examples. For this, autoencoder was trained to find and delete similar points from the training dataset. De-duplication process helped in removing 275k images.\n\n\n\n\n\nGenerated bedrooms after five epochs of training source\n\n\n\n\n\nGenerated bedrooms after five epochs of training source\n\n\nReferences\n\nUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\nHenry AI Labs"
  },
  {
    "objectID": "posts/2020-08-04-general-adversarial-networks-gans.html",
    "href": "posts/2020-08-04-general-adversarial-networks-gans.html",
    "title": "General Adversarial Networks (GANs)",
    "section": "",
    "text": "“General Adversarial Nets is the most interesting idea in the last 10 years in machine learning”. This was the statement from Yann LeCun regarding GANs when Ian Goodfellow and co-authors introduced it in 2014. After its first introduction, many research papers are published with various architectures and its use cases.\nSo what are General Adversarial Networks? What are its use cases. In this post I will try to explain about GANs, its underlying math, use cases and GAN implementation in keras."
  },
  {
    "objectID": "posts/2020-08-04-general-adversarial-networks-gans.html#introduction",
    "href": "posts/2020-08-04-general-adversarial-networks-gans.html#introduction",
    "title": "General Adversarial Networks (GANs)",
    "section": "Introduction",
    "text": "Introduction\nAs stated by Ian Goodfellow on his paper, GAN is a framework for estimating generative models via an adversarial process. During this process, two models are trained. One is called generator \\(G\\) and another model is called as discriminator \\(D\\). Generator \\(G\\) generates new examples that are similar to original data. Discriminator model \\(D\\) classifies whether the data is real or fake. To keep in simple terms, generator is analogous to counterfeiters, whereas discriminator is analogous to police. Counterfeiters tries to produce fake currency and use it, while police try to detect the fake currency. Counterfeiters come up with new ideas and patterns to make the fake money as similar to the original and fool the police. Similarly, police tries to detect the fake money. Similar is the case with GAN. Generative model tries to create fake data samples and fool the discriminator and discriminator classifies whether data is fake or not. This process goes on until data samples generated by generator are indistinguishable from discriminator.\nConsider the following notations for different data points and distributions:\nGenerator’s distribution: \\(p_g\\)\nData: \\(x\\)\nInput noise variables: \\(p_z(z)\\)\nThen, \\(G\\) is a generator model represented by multilayer perceptron with parameters \\(\\theta_g\\) (parameters of weights and biases).\nSimilarly, \\(D\\) is a discriminator model also represented by multilayer perceptron \\(D(x;\\theta\\_d)\\). Then, \\(D(x)\\) represents a probability that data \\(x\\) came from original distribution rather than \\(p_g\\).\nA known dataset serves as input for the discriminator. Training involves presenting samples from the training dataset until it achieves acceptable accuracy. Generator however trains based on whether it fools the discriminator. The input to generator is the data samples from latent space ( e.g, multivariate normal distribution). Then, the output generated by the generator is evaluated by the discriminator. Both generator and discriminator model goes through backpropagation to reduce the loss. During this step, generator generates better data samples (say images), whereas discriminator becomes good in classifying fake samples coming from the generator. This way, discriminator \\(D\\) and generator \\(G\\) play two-player min-max game."
  },
  {
    "objectID": "posts/2020-08-04-general-adversarial-networks-gans.html#training-procedure-for-gans",
    "href": "posts/2020-08-04-general-adversarial-networks-gans.html#training-procedure-for-gans",
    "title": "General Adversarial Networks (GANs)",
    "section": "Training procedure for GANs",
    "text": "Training procedure for GANs\n\nTake a random noise vector \\(z\\) and feed to the generator \\(G\\) to produce fake examples \\(x^*\\). Here label y=0 for \\((x, y)\\) input-output pair.\nTake fake data \\(x^*\\) and real data \\(x\\) and feed to the discriminator model alternatively.\nSince, discriminator \\(D\\) is a multilayer perceptron, it outputs value between 0 and 1. These values indicates the probability that input is real.\nBoth generator and discriminator calculates their respective loss and perform backpropagation to reduce the loss.\nDiscriminator tries to maximize the probability of assigning correct labels to both original data and data from random samples.\nSimilarly, generator tries to minimize the discriminator’s ability to detect correct and fake samples.\n\nThese two networks go on competing with each other until they reach Nash equilibrium. Nash equilibrium is a point in a game where neither player can improve their situation by changing their strategy. More on Nash equilibrium can be found here. The overview of GAN architecture is shown below:\n\n\n\n\nFig. GAN\n\nThe noise vector z is transformed into x* by a generator model which is then fed into discriminator network. Similarly, data from original sample is also fed into discriminator. The discriminator in result outputs a classification values close to 1 for real data x. While for data x*, discriminator tries to output value 0 indicating that x* is fake."
  },
  {
    "objectID": "posts/2020-08-04-general-adversarial-networks-gans.html#derivation-of-loss-function-for-gans",
    "href": "posts/2020-08-04-general-adversarial-networks-gans.html#derivation-of-loss-function-for-gans",
    "title": "General Adversarial Networks (GANs)",
    "section": "Derivation of Loss function for GANs",
    "text": "Derivation of Loss function for GANs\nSince, GAN is trained in multilayer perception, its loss can be calculated using cross-entropy loss given as:\n\\(L(y, y\\hat{})\\) = \\([y\\log y\\hat{} + (1-y) log(1-y\\hat{} )]\\)\nThe label for the data coming from \\(p_{data}(x)\\) is \\(y = 1\\) and \\(y\\hat{}\\) = \\(D(x)\\).\nSo, the cross-entropy equation becomes: \\(L(D(x), 1)\\) = \\(log(D(x))\\) ————–(A)\nSimilarly, for data coming from generator the label is \\(y=0\\) and \\(y\\hat{}=D(G(z))\\)\nIn this case, our cross entropy equation becomes: \\(L(D(G(z)), 0)\\) = \\((1-0) log(1-D(G(z))\\) = \\(log(1-D(G(z))\\) ————–(B)\nWe know that the objective of discriminator is to correctly classify fake versus real data. To achieve this equations (A) and (B) should be maximized.\n\\(max\\) { \\(log (D(x))\\) + \\(log(1-D(G(z)))\\)}\nThe role of generator is to fool discriminator so as to predict fake data as real, i.e., to achieve \\(D(G(z)) = 1\\) . So, the objective function for generator is given as:\n\\(min\\) {\\(log (D(x))\\) + \\(log(1-D(G(z)))\\)}\nNote: \\(log(D(x))\\) has nothing to do with generator objective function. It is kept to provide compact representation of generator and discriminator objective function in our equation.\nCombining the objective function of both discriminator and generator, we get following equation:\n\\(\\min\\_{G}\\) \\(\\max\\_{D}\\) {\\(log (D(x))\\) + \\(log(1-D(G(z)))\\)}\nAll the above equations are written with respect to a single instance of data point (x). To consider all the instances of x, we need to take expectation of the whole arguments present in the equation which results in the following equation:"
  },
  {
    "objectID": "posts/2020-08-04-general-adversarial-networks-gans.html#applications-of-gans",
    "href": "posts/2020-08-04-general-adversarial-networks-gans.html#applications-of-gans",
    "title": "General Adversarial Networks (GANs)",
    "section": "Applications of GANs",
    "text": "Applications of GANs\n\nImage-to-Image Translation\nImage-to-Text Translation\nTo generate realistic photographs\nPhoto Inpainting and many more"
  },
  {
    "objectID": "posts/2020-08-04-general-adversarial-networks-gans.html#implementation-of-gan",
    "href": "posts/2020-08-04-general-adversarial-networks-gans.html#implementation-of-gan",
    "title": "General Adversarial Networks (GANs)",
    "section": "Implementation of GAN",
    "text": "Implementation of GAN\nSince, GANs consists of two models, generator and discriminator model, we need to build two models. Before building models let’s import libraries. The code snippets for GAN implementation is taken from GANs in Action book.\nfrom keras.datasets import mnist\nfrom keras.layers import Dense, Flatten, Reshape\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nSince we will use mnist data to train our discriminator, we need 28*28 image size for our generator for generating new images.\nimg_rows = 28\nimg_cols = 28\n\nchannels = 1\nimg_shape = (img_rows, img_cols, channels)\n\nz_dim  = 100\nz_dim is the size of noise vector used as input to generator model\nNow, we’ll build a generator model\ndef build_generator(img_shape, z_dim):\n  model = Sequential()\n\n  model.add(Dense(128, input_dim=z_dim))\n  model.add(LeakyReLU(alpha=0.01))\n  model.add(Dense(28*28*1, activation='tanh'))\n  model.add(Reshape(img_shape))\n  return model\nSimilarly, building generator model\ndef build_discriminator(img_shape):\n  model = Sequential()\n  model.add(Flatten(input_shape=img_shape))\n  model.add(Dense(128))\n  model.add(LeakyReLU(alpha=0.01))\n  model.add(Dense(1, activation='sigmoid'))\n  return model\nNow, we will build GAN using generator and discriminator build previously. While using combined model to train generator, we keep the parameters of discriminator model fixed. Also, discriminator is trained as an independently compiled model.\ndef build_gan(generator, discriminator):\n  model = Sequential()\n  model.add(generator)\n  model.add(discriminator)\n\n  return model\n\ndiscriminator = build_discriminator(img_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n\ngenerator = build_generator(img_shape, z_dim)\ndiscriminator.trainable = False\ngan = build_gan(generator, discriminator)\ngan.compile(loss='binary_crossentropy', optimizer=Adam())\nNow, that we have build our GAN model, we now train our GAN model. MNIST images is taken as real examples and fake image is generated from noise vector z. These are used to train discriminator network while keeping generator’s parameters constant. Similarly, fake images are generated and we used those images to train generator network by keeping discriminator’s parameter constant.\nThe images produced by generator over the course of training iterations is shown below\n\n\n\nDuring training, random noise is generated and generator gradually learns to imitate the features of training dataset.\n\n\n\nThis is the output from two layer general adversarial networks. It is gradually imitating the features of MNIST images.\n\nReferences\n\nIan J. Goodfellow et. al. Generative Adversarial Nets.\nhttps://en.wikipedia.org/wiki/Generative_adversarial_network\nAhlad Kumar (GAN youtube playlist)"
  },
  {
    "objectID": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html",
    "href": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html",
    "title": "Autocorrect and Minimum Edit Distance",
    "section": "",
    "text": "This is my brief note from DeepLearning.AI’s NLP Specialization Course."
  },
  {
    "objectID": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html#what-is-autocorrect",
    "href": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html#what-is-autocorrect",
    "title": "Autocorrect and Minimum Edit Distance",
    "section": "What is Autocorrect?",
    "text": "What is Autocorrect?\nAutocorrect is an application that changes misspelled word into a correct word. When writing messages or drafting an email, you may have noticied that if we type any words that is misspelled, then that word automatically gets corrected with correct spelling and based on the context.\n\n\n\n\nFig. Autocorrect in action in google document"
  },
  {
    "objectID": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html#how-does-autocorrect-work",
    "href": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html#how-does-autocorrect-work",
    "title": "Autocorrect and Minimum Edit Distance",
    "section": "How does autocorrect work??",
    "text": "How does autocorrect work??\nWhile typing the document we can see we get automatic correction in our document. The basic working of this automatic correction is:\n\nIdentifying a misspelled word\nFind the strings n edit distance away\nFilter the candidates\nCalculate word probabilities\n\nNow let’s see each of the points in detail.\n\n1. Identifying a misspelled word:\nLet’s say we are writing a sentence This is a draft docment of the APIs. Here we can see clearly see that the word docment is misspelled. But, how do we know that this is a misspelled word? Well, we will have a dictionary containing all correct words and if we do not encounter given string in the dictionary, that string is obviously a misspelled word.\nif word not in vocab:\n  misspelled = True       # If the word is not in vocab, we identify it as a misspelled word.\nWhile identifying a misspelled words, we are only looking at the vocab but not the context. Consider a sentence Hello deah. Here, dear is misspelled as deah. If we write deer instead of dear, then we would not be able to identify misspelled word because deer is present in vocab, though it is contextually incorrect.\n\n\n2. Find strings n edit distance away\nEdit is an operation that is performed on a string to change it.\n\nTypes of edit: - Insert (add a letter) ‘to’: ‘top’, ‘two’ - Delete (remove a letter) ‘hat’: ‘ha’, ‘at’, ‘ht’ - Switch (swap 2 adjacent letters) ‘eta’: ‘eat’, ‘tea’ - Replace (change 1 letter to another) ‘jaw’: ‘jar’, ‘paw’ Using these edits, we can find all possible strings that are n edits away.\n\n\n\n3. Filter candidates\nAfter findings strings that are n edit distance away, next step is to filter those strings. After applying edits, the strings are compared with the vocab, and if those strings are not present in vocab, they are discarded. This, way we get a list of actual words.\n\n\n4. Calculate the word probabilities\nThe final step is to calculate the word probabilities and find the most likely word from the vocab. Given the sentence I am learning AI because AI is the new electricity, we find occurrence of each word and calculate the probability. Probability of given word w can be calculated as the ratio of the count of word w to the total size of the corpus. Mathematically: \\(P(w) = \\frac{C(w)}{V}\\),\nwhere:\n\n\\(P(w) - Probability \\ of\\ a\\ word\\)\n\\(C(w) - Number \\ of \\ times \\ the \\ word \\ appears\\)\n\\(V - Total \\ size \\ of \\ the \\ corpus\\)"
  },
  {
    "objectID": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html#minimum-edit-distance",
    "href": "posts/2021-10-25-autocorrect-and-minimum-edit-distance.html#minimum-edit-distance",
    "title": "Autocorrect and Minimum Edit Distance",
    "section": "Minimum Edit Distance",
    "text": "Minimum Edit Distance\nTill now, we have seen how edit distance works and what are its applications in NLP domain. Now let’s look at the minimum edit distance.\nMinimum edit distance is the minimum number of edits required to transform one string to another. It is used in various applications such as spelling correction, machine translation, DNA sequencing, and many more. To calculate minimum edit distance, we use three types of operations which is also discussed above, i.e., insert, delete and replace.\nFor example: Consider source word deer and target word door. To change source to target, we need to perform two replace operations i.e., replace each of e to o. Here number of edits is 2, but in minimum distance distance, there are cost associated with different edit operations.\n\n\n\nEdit Operations\nCost\n\n\n\n\nInsert\n1\n\n\nDelete\n1\n\n\nReplace\n2\n\n\n\nUsing above table, the edit distance for the above problem is: Edit distance = 2 x replace cost = 2x2 = 4\nThis is a brute force method where we are simply looking at the source and target word to calculate the edit distance. If we have large sentence, calculating edit distance with mentioned approach becomes tedious. To make things simpler, we can opt for tabular method using Dynamic Programming approaches."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html",
    "title": "Illustrated Vision Transformers",
    "section": "",
    "text": "Ever since Transformer was introduced in 2017, there has been a huge success in the field of Natural Language Processing (NLP). Almost all NLP tasks use Transformers and it’s been a huge success. The main reason for the effectiveness of the Transformer was its ability to handle long-term dependencies compared to RNNs and LSTMs. After its success in NLP, there have been various approaches to its usage for Computer Vision tasks. This paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. proposes using the transformer and has achieved some great results in various Computer Vision tasks.\nVison Transformer (ViT) makes use of an extremely large dataset while training the model. While training on datasets such as ImageNet (paper labels ImageNet as a mid-sized dataset), the accuracies of the model fall below ResNets. This is because the Transformer lack inductive bias such as translation equivariance and locality, thus it does not generalize well when trained on insufficient data.\n\n\n\nSplit image into patches\nProvide sequence of linear embeddings of these patches as an input to transformer (flattening the image) Here, image patches are treated as the same way as tokens (as in NLP tasks)\nAdd positional embeddings and a learnable embedding class (similar to BERT) to each patch embeddings\nPass these (patch + positional + class] embeddings through Transformer encoder and get the output values for each class tokens\nPass the representation of class through MLP head and get the final class predictions."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#method",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#method",
    "title": "Illustrated Vision Transformers",
    "section": "Method",
    "text": "Method\n\n\nSource: Google AI Blog\n\nFigure above depicts the overview of Vision Transformer. As shown in the figure, given the image, the image is split into patches. These image patches are flattened and passed to transformer encoder a sequence of tokens. Along with patch embeddings, position embedding is also passed as an input to the transformer encoder. Here position embedding is added along with patch embedding to retain positional information."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#how-is-an-image-changed-into-a-sequence-of-vectors-to-feed-into-the-transformer-encoder",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#how-is-an-image-changed-into-a-sequence-of-vectors-to-feed-into-the-transformer-encoder",
    "title": "Illustrated Vision Transformers",
    "section": "How is an image changed into a sequence of vectors to feed into the transformer encoder?",
    "text": "How is an image changed into a sequence of vectors to feed into the transformer encoder?\nLet’s decode above figure by taking a RGB image of size \\(256 * 256 * 3\\). The first step is to create patches of size \\(16 * 16\\) from input image. We can create \\(16 * 16 = 256\\) total patches. After splitting input images into patches, another step is to lineary place all splitted images. As seen in the figure, first patch is placed on the left most side and right most on the far right. Then, we linearly project these patches to get \\(1 * 768\\) vector representations. These representation is known as patch embeddings. The size of patch embedding becomes \\(256 * 768\\) (since we have 256 total patches with each patch represented as \\(1 * 768\\) vector.\nNext, we prepend learnable embedding class token and position embeddings along with patch embeddings making the size \\(257 * 768\\). Here, position embeddings are used to retain positional information. After converting images into vector representation, we need to send image in order as transformer doesnot know the order of the patches unlike CNNs. Due to this, we need to manually add some information about the position of the patches."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#components-of-vision-transformer",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#components-of-vision-transformer",
    "title": "Illustrated Vision Transformers",
    "section": "Components of Vision Transformer",
    "text": "Components of Vision Transformer\nSince Vision Transformer is based on standard transformer architecture, only difference it being used for image tasks rather than for text, components used here is almost the same. Here, we discuss the components used in Vision transformer along with its significance.\n\nSide note: If you want to dive deep into transformer, then here by Jay Alammar is a good place to start with.\n\n\nPatch embeddings\nAs the name of the paper “An Image is worth \\(16 * 16\\) words transformers”, the main take away of the paper is the breakdown of images into patches. Given the image: \\(x \\, \\varepsilon \\, \\mathbb{R}^{(H * W * C)}\\) it is reshaped into 2D flattened patches \\(x_p \\, \\varepsilon \\, \\mathbb{R}^{N*(P^2.C))}\\), where, N=\\(\\frac{H.W}{p^2}\\), \\((P, P)\\) is the resolution of each image patch."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#learnable-embedding-class",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#learnable-embedding-class",
    "title": "Illustrated Vision Transformers",
    "section": "Learnable embedding class",
    "text": "Learnable embedding class\nA learnable embeding is added to the embeded patches \\(z_0^0 = x_{class}\\). The state of this embedding class at the output of Transformer encoder \\(z_L^0\\) serves as the representation \\(y\\). This classification head is attached to \\(z_L^0\\) during both pre-training and fine-tuning."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#position-embeddings",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#position-embeddings",
    "title": "Illustrated Vision Transformers",
    "section": "Position Embeddings",
    "text": "Position Embeddings\nPosition Embeddings are added to the patch embeddings along with class token which are then fed into the transformer encoder."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#transformer-encoder",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#transformer-encoder",
    "title": "Illustrated Vision Transformers",
    "section": "Transformer Encoder",
    "text": "Transformer Encoder\n\nThe transformer encoder is a standard transformer encoder architecture as presented in original transformer paper. This encoder takes embedded patches (patch embedding, position embedding and class embedding). The transformer encoder consists of alternating layers of multuheaded self-attention and MLP blocks. Layer Normalization is used before every block and residual connection is used after every block."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#using-hybrid-architecture",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#using-hybrid-architecture",
    "title": "Illustrated Vision Transformers",
    "section": "Using hybrid architecture",
    "text": "Using hybrid architecture\nPreviously, image patches were used to form input sequence, another approach to form input sequence can be the feature map of a CNN (Convolution Neural Network). Here, the patches extracted from CNN map is used as patch embedding. From the paper:\n\nAs an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN. In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension."
  },
  {
    "objectID": "posts/2021-07-27-illustrated-vision-transformers.html#references",
    "href": "posts/2021-07-27-illustrated-vision-transformers.html#references",
    "title": "Illustrated Vision Transformers",
    "section": "References",
    "text": "References\n\nAn image is worth 16 * 16 words: Transformers for image recognition at scale\nViT Blog - Aman Arora\nThe AI Summer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prabin Nepal",
    "section": "",
    "text": "Brief overview of GPT-4\n\n\n\n\n\n\n\nmachine-learning\n\n\nNLP\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nText Summarization NLP\n\n\n\n\n\n\n\nNLP\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAutocorrect and Minimum Edit Distance\n\n\n\n\n\n\n\nNLP\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\nIllustrated Vision Transformers\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPaper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)\n\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2021\n\n\n\n\n\n\n  \n\n\n\n\nDeep Residual Learning for Image Recognition (ResNet paper explained)\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\nSelf-supervised Learning\n\n\n\n\n\n\n\ndeep-learning\n\n\nself-supervised-learning\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMobileNet Architecture Explained\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nNeural style transfer and its working\n\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\nDeep Convolutional Generative Adversarial Networks (DCGANs)\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\nGeneral Adversarial Networks (GANs)\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\nPaper Explanation: Going deeper with Convolutions (GoogLeNet)\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nVGGNet Architecture Explained\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAlexNet Architecture Explained\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeep-learning\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Prabin Nepal",
    "section": "",
    "text": "Languages: Python, JavaScript\nDatabases: MySQL, Postgresql, MongoDB\nFrameworks and Libraries: Pytorch, Tensorflow, Huggingface Transformers, OpenCV, SpaCy, Django, FastAPI, Vuejs\nTools: Git, Linux, Docker, AWS (EC2, S3, Lambda function, Sagemaker), Hydra, Model Packaging (ONXX)"
  }
]